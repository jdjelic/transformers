{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.8.0 torch==1.7.1 pytorch-lightning==1.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC_dpogPzzSJ",
        "outputId": "91386ffa-1b69-4307-eaa1-1040a1b1ebcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "  Downloading torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 14 kB/s \n",
            "\u001b[?25hCollecting pytorch-lightning==1.2.2\n",
            "  Downloading pytorch_lightning-1.2.2-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.2.0)\n",
            "Collecting PyYAML!=5.4.*,>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 92.9 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 72.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.2) (2.8.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 75.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.44.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.2) (57.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.2) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 90.6 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 89.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning==1.2.2) (21.4.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=10237384b7c2f915514563e649e245fc3ac573cf632e2e03bf16f2857ee94f25\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torch, PyYAML, future, torchtext, pytorch-lightning\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.3.0 future-0.18.2 multidict-6.0.2 pytorch-lightning-1.2.2 torch-1.7.1 torchtext-0.8.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQNLtiYRLMsB"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "nn_Softargmax = nn.Softmax "
      ],
      "metadata": {
        "id": "oKWg9OtmLtQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz5Vl7AbUzGk",
        "outputId": "f8217315-00eb-45d7-e7a0-659490075663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_input=None): # p, \n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        if d_input is None:\n",
        "            d_xq = d_xk = d_xv = d_model\n",
        "        else:\n",
        "            d_xq, d_xk, d_xv = d_input\n",
        "            \n",
        "         # Make sure that the embedding dimension of model is a multiple of number of heads\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.d_k = d_model // self.num_heads\n",
        "        \n",
        "        # These are still of dimension d_model. They will be split into number of heads \n",
        "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
        "        \n",
        "        # Outputs of all sub-layers need to be of dimension d_model\n",
        "        self.W_h = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        batch_size = Q.size(0) \n",
        "        k_length = K.size(-2) \n",
        "        \n",
        "        # Scaling by d_k so that the soft(arg)max doesnt saturate\n",
        "        Q = Q / np.sqrt(self.d_k)                         # (bs, n_heads, q_length, dim_per_head)\n",
        "        scores = torch.matmul(Q, K.transpose(2,3))          # (bs, n_heads, q_length, k_length)\n",
        "        \n",
        "        A = nn_Softargmax(dim=-1)(scores)   # (bs, n_heads, q_length, k_length)\n",
        "        \n",
        "        # Get the weighted average of the values\n",
        "        H = torch.matmul(A, V)     # (bs, n_heads, q_length, dim_per_head)\n",
        "\n",
        "        return H, A \n",
        "\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (heads X depth)\n",
        "        Return after transpose to put in shape (batch_size X num_heads X seq_length X d_k)\n",
        "        \"\"\"\n",
        "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def group_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Combine the heads again to get (batch_size X seq_length X (num_heads times d_k))\n",
        "        \"\"\"\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "    \n",
        "\n",
        "    def forward(self, X_q, X_k, X_v):\n",
        "        batch_size, seq_length, dim = X_q.size()\n",
        "\n",
        "        # After transforming, split into num_heads \n",
        "        Q = self.split_heads(self.W_q(X_q), batch_size)  # (bs, n_heads, q_length, dim_per_head)\n",
        "        K = self.split_heads(self.W_k(X_k), batch_size)  # (bs, n_heads, k_length, dim_per_head)\n",
        "        V = self.split_heads(self.W_v(X_v), batch_size)  # (bs, n_heads, v_length, dim_per_head)\n",
        "        \n",
        "        # Calculate the attention weights for each of the heads\n",
        "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
        "        \n",
        "        # Put all the heads back together by concat\n",
        "        H_cat = self.group_heads(H_cat, batch_size)    # (bs, q_length, dim)\n",
        "        \n",
        "        # Final linear layer  \n",
        "        H = self.W_h(H_cat)          # (bs, q_length, dim)\n",
        "        \n",
        "        return H, A"
      ],
      "metadata": {
        "id": "pzEl3G6AL3ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Some sanity checks:"
      ],
      "metadata": {
        "id": "RXCna-024oGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=64)\n",
        "def print_out(Q, K, V):\n",
        "    temp_out, temp_attn = temp_mha.scaled_dot_product_attention(Q, K, V)\n",
        "    print('Attention weights are:', temp_attn.squeeze())\n",
        "    print('Output is:', temp_out.squeeze())\n"
      ],
      "metadata": {
        "id": "P1C02Ojt4yIv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_K = torch.tensor(\n",
        "    [[10, 0, 0],\n",
        "     [ 0,10, 0],\n",
        "     [ 0, 0,10],\n",
        "     [ 0, 0,10]]\n",
        ").float()[None,None]\n",
        "\n",
        "test_V = torch.tensor(\n",
        "    [[   1,0,0],\n",
        "     [  10,0,0],\n",
        "     [ 100,5,0],\n",
        "     [1000,6,0]]\n",
        ").float()[None,None]\n",
        "\n",
        "test_Q = torch.tensor(\n",
        "    [[0, 0, 10]]\n",
        ").float()[None,None]\n",
        "print_out(test_Q, test_K, test_V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-LsQi-Z5upe",
        "outputId": "d7e01721-ca0c-4d61-ffb6-5adf2632c222"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are: tensor([2.2097e-16, 2.2097e-16, 5.0000e-01, 5.0000e-01])\n",
            "Output is: tensor([550.0000,   5.5000,   0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_Q = torch.tensor(\n",
        "    [[0, 0, 10], [0, 10, 0], [10, 10, 0]]\n",
        ").float()[None,None]\n",
        "print_out(test_Q, test_K, test_V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjERiwHaFSs-",
        "outputId": "0ec90a89-c230-446a-f55c-01adec58dce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are: tensor([[1.8633e-06, 1.8633e-06, 5.0000e-01, 5.0000e-01],\n",
            "        [3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06],\n",
            "        [5.0000e-01, 5.0000e-01, 1.8633e-06, 1.8633e-06]])\n",
            "Output is: tensor([[5.5000e+02, 5.5000e+00, 0.0000e+00],\n",
            "        [1.0004e+01, 4.0993e-05, 0.0000e+00],\n",
            "        [5.5020e+00, 2.0497e-05, 0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1D convolution with kernel_size = 1\n",
        "# This is basically an MLP with one hidden layer and ReLU activation applied to each and every element in the set!!!"
      ],
      "metadata": {
        "id": "kGlBjtWXHpnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim, p):\n",
        "        super().__init__()\n",
        "        self.k1convL1 = nn.Linear(d_model,    hidden_dim)\n",
        "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.k1convL1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.k1convL2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3swcYN-jHrcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer encoder\n",
        "#Now we have all components for our Transformer Encoder block shown below!!!!"
      ],
      "metadata": {
        "id": "ul0bWDPgH_u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, conv_hidden_dim, p=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, p)\n",
        "        self.cnn = CNN(d_model, conv_hidden_dim, p)\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Multi-head attention \n",
        "        attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        # Layer norm after adding the residual connection \n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        # Feed forward \n",
        "        cnn_output = self.cnn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        #Second layer norm after adding residual connection \n",
        "        out2 = self.layernorm2(out1 + cnn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "metadata": {
        "id": "JHw-48rBIHHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
        "    theta = np.array([\n",
        "        [p / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
        "        for p in range(nb_p)\n",
        "    ])\n",
        "    E[:, 0::2] = torch.FloatTensor(np.sin(theta[:, 0::2]))\n",
        "    E[:, 1::2] = torch.FloatTensor(np.cos(theta[:, 1::2]))\n",
        "    #E = E.detach()\n",
        "    #E.requires_grad = False\n",
        "    E = E.to(device)\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, max_position_embeddings, p):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
        "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
        "        create_sinusoidal_embeddings(\n",
        "            nb_p=max_position_embeddings,\n",
        "            dim=d_model,\n",
        "            E=self.position_embeddings.weight\n",
        "        )\n",
        "\n",
        "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) # (max_seq_length)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)                      # (bs, max_seq_length)\n",
        "        \n",
        "        # Get word embeddings for each input id\n",
        "        word_embeddings = self.word_embeddings(input_ids)                   # (bs, max_seq_length, dim)\n",
        "        \n",
        "        # Get position embeddings for each position id \n",
        "        position_embeddings = self.position_embeddings(position_ids)        # (bs, max_seq_length, dim)\n",
        "        \n",
        "        # Add them both \n",
        "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
        "        \n",
        "        # Layer norm \n",
        "        embeddings = self.LayerNorm(embeddings)             # (bs, max_seq_length, dim)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "8NbNq-bwMA0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, ff_hidden_dim, input_vocab_size,\n",
        "               maximum_position_encoding):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embeddings(d_model, input_vocab_size,maximum_position_encoding)\n",
        "\n",
        "        self.enc_layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.enc_layers.append(EncoderLayer(d_model, num_heads, ff_hidden_dim, p))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "id": "j_D3-rPczcc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchtext import legacy\n",
        "from torchtext import data\n",
        "import torchtext.datasets as datasets\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "tuGY9KsKziOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 200\n",
        "text = data.Field(sequential=True, fix_length=max_len, batch_first=True, lower=True, dtype=torch.long)\n",
        "label = data.LabelField(sequential=False, dtype=torch.long)\n",
        "ds_train, ds_test = datasets.IMDB.splits(text, label, root='./')\n",
        "print('train : ', len(ds_train))\n",
        "print('test : ', len(ds_test))\n",
        "print('train.fields :', ds_train.fields)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0LuPbkW13pA",
        "outputId": "ff012bfc-a81f-49f4-f52e-56a042677cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train :  25000\n",
            "test :  25000\n",
            "train.fields : {'text': <torchtext.data.field.Field object at 0x7f4c5b315250>, 'label': <torchtext.data.field.LabelField object at 0x7f4c5b3152d0>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train, ds_valid = ds_train.split(0.9)\n",
        "print('train : ', len(ds_train))\n",
        "print('valid : ', len(ds_valid))\n",
        "print('test : ', len(ds_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAa00QdI2YFW",
        "outputId": "6ffd368e-dd75-4be3-e9fa-63250d948662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train :  22500\n",
            "valid :  2500\n",
            "test :  25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 50_000\n",
        "text.build_vocab(ds_train, max_size=num_words)\n",
        "label.build_vocab(ds_train)\n",
        "vocab = text.vocab"
      ],
      "metadata": {
        "id": "LiYTiAOi3CQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 164\n",
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "    (ds_train, ds_valid, ds_test), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBDAaIfH3jVI",
        "outputId": "730ae816-9b19-42b6-9375-894571ca842b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size, num_answers):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size,\n",
        "                         maximum_position_encoding=10000)\n",
        "        self.dense = nn.Linear(d_model, num_answers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        \n",
        "        x, _ = torch.max(x, dim=1)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EQnk_8onT1VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerClassifier(num_layers=1, d_model=32, num_heads=2, \n",
        "                         conv_hidden_dim=128, input_vocab_size=50002, num_answers=2)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxGlJpntUiUR",
        "outputId": "204778a5-5e62-4d68-df46-b8159eeeafe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:390: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if param.grad is not None:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerClassifier(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embeddings(\n",
              "      (word_embeddings): Embedding(50002, 32, padding_idx=1)\n",
              "      (position_embeddings): Embedding(10000, 32)\n",
              "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (enc_layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (mha): MultiHeadAttention(\n",
              "          (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "        (cnn): CNN(\n",
              "          (k1convL1): Linear(in_features=32, out_features=128, bias=True)\n",
              "          (k1convL2): Linear(in_features=128, out_features=32, bias=True)\n",
              "          (activation): ReLU()\n",
              "        )\n",
              "        (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "t_total = len(train_loader) * epochs"
      ],
      "metadata": {
        "id": "uKJMfvChWUn-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "00afe880-f5e9-4504-e335-bdcc99e2f2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-2d23c8235838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     45\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     46\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    231\u001b[0m                                 \"but one of the params is \" + torch.typename(param))\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can't optimize a non-leaf Tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: can't optimize a non-leaf Tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, valid_loader):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
        "        nb_batches_train = len(train_loader)\n",
        "        train_acc = 0\n",
        "        model.train()\n",
        "        losses = 0.0\n",
        "\n",
        "        for batch in train_iterator:\n",
        "            x = batch.text.to(device)\n",
        "            y = batch.label.to(device)\n",
        "            \n",
        "            out = model(x)  # ①\n",
        "\n",
        "            loss = f.cross_entropy(out, y)  # ②\n",
        "            \n",
        "            model.zero_grad()  # ③\n",
        "\n",
        "            loss.backward()  # ④\n",
        "            losses += loss.item()\n",
        "\n",
        "            optimizer.step()  # ⑤\n",
        "                        \n",
        "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
        "        \n",
        "        print(f\"Training loss at epoch {epoch} is {losses / nb_batches_train}\")\n",
        "        print(f\"Training accuracy: {train_acc / nb_batches_train}\")\n",
        "        print('Evaluating on validation:')\n",
        "        evaluate(valid_loader)"
      ],
      "metadata": {
        "id": "MDTRrCiNYAoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader):\n",
        "    data_iterator = iter(data_loader)\n",
        "    nb_batches = len(data_loader)\n",
        "    model.eval()\n",
        "    acc = 0 \n",
        "    for batch in data_iterator:\n",
        "        x = batch.text.to(device)\n",
        "        y = batch.label.to(device)\n",
        "                \n",
        "        out = model(x)\n",
        "        acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
        "\n",
        "    print(f\"Eval accuracy: {acc / nb_batches}\")"
      ],
      "metadata": {
        "id": "Fpg48iRaYHqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_loader, valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVTtsBa7YNxT",
        "outputId": "f0ebfa3b-e5e0-40ab-a1ff-adc736ce5d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss at epoch 0 is 0.707829168727321\n",
            "Training accuracy: 0.5183229437753435\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.5549075769208055\n",
            "Training loss at epoch 1 is 0.6555137850584523\n",
            "Training accuracy: 0.6173301357703401\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6774715053445933\n",
            "Training loss at epoch 2 is 0.5598647020036175\n",
            "Training accuracy: 0.7130275474590974\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7348889741924055\n",
            "Training loss at epoch 3 is 0.45274748172490825\n",
            "Training accuracy: 0.790301145877464\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7798750959664559\n",
            "Training loss at epoch 4 is 0.3707184892508291\n",
            "Training accuracy: 0.8361955052553006\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7966175515266046\n",
            "Training loss at epoch 5 is 0.30595145182263467\n",
            "Training accuracy: 0.8693538300147268\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.8210816157798383\n",
            "Training loss at epoch 6 is 0.24821411001105462\n",
            "Training accuracy: 0.9017152857632796\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.8292609106478472\n",
            "Training loss at epoch 7 is 0.201354589193098\n",
            "Training accuracy: 0.9231550969355847\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.8279542904387881\n",
            "Training loss at epoch 8 is 0.16125327012231272\n",
            "Training accuracy: 0.9419105186709433\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.8385770389180889\n",
            "Training loss at epoch 9 is 0.12560926675195655\n",
            "Training accuracy: 0.9583396376767738\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.8379717120415755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.stoi[\"awdfufl\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwV4xe9ge8p2",
        "outputId": "52a55679-2336-463b-e34e-08dc1f7b8290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i =0\n",
        "hate = 782\n",
        "this = 10\n",
        "film = 24\n",
        "is = 7\n",
        "great = 87\n",
        "awful = 553"
      ],
      "metadata": {
        "id": "ICT4JRNNhoKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input1 = torch.LongTensor([[10,24, 7, 553]])\n",
        "\n",
        "input1 = input1.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97HOk06LtasF",
        "outputId": "647ef288-601b-4aee-b2d9-e9cbd8286c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerClassifier(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embeddings(\n",
              "      (word_embeddings): Embedding(50002, 32, padding_idx=1)\n",
              "      (position_embeddings): Embedding(10000, 32)\n",
              "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (enc_layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (mha): MultiHeadAttention(\n",
              "          (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "        (cnn): CNN(\n",
              "          (k1convL1): Linear(in_features=32, out_features=128, bias=True)\n",
              "          (k1convL2): Linear(in_features=128, out_features=32, bias=True)\n",
              "          (activation): ReLU()\n",
              "        )\n",
              "        (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAVSeOUeefcm",
        "outputId": "86721975-3b48-4588-a070-5af562371f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10,  7,  3]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(input1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iQa2xaLce6D",
        "outputId": "b0609f67-68ed-4282-808b-71d0cbba16c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.5357, -3.8993]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in train_loader:\n",
        "  print(data.text)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n2Otb2Rt-P7",
        "outputId": "0fca71b0-bfe0-4f6b-ac43-7527ca8c71ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   10,     7,     3,  ...,     1,     1,     1],\n",
            "        [ 5377, 37240,  2356,  ...,    14,    32,  6675],\n",
            "        [    2,    24,   339,  ...,   296,     0,  5366],\n",
            "        ...,\n",
            "        [  311,    10,   215,  ...,     1,     1,     1],\n",
            "        [   85,    72,    87,  ...,     1,     1,     1],\n",
            "        [ 1586,     9,   370,  ...,     1,     1,     1]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1mtnKenYflq",
        "outputId": "dce017a2-b86b-434f-9332-7cbcbed5958a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval accuracy: 0.8158532157216994\n"
          ]
        }
      ]
    }
  ]
}
